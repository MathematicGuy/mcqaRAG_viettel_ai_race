{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "159b79de",
   "metadata": {},
   "source": [
    "# MCQ Inference Notebook - Direct llama.cpp Client\n",
    "\n",
    "This notebook uses `OllamaCppClient` directly for MCQ answering.\n",
    "\n",
    "## Architecture\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Notebook (run_inference.ipynb)                 â”‚\n",
    "â”‚  â”œâ”€ ask_single_question_cpp()                   â”‚\n",
    "â”‚  â””â”€ Direct import from ollamacpp_client.py      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                     â”‚\n",
    "                     â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  OllamaCppClient (ollamacpp_client.py)          â”‚\n",
    "â”‚  â”œâ”€ generate_mcq_answer()                       â”‚\n",
    "â”‚  â”œâ”€ _build_mcq_prompt()                         â”‚\n",
    "â”‚  â””â”€ _parse_mcq_response()                       â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                     â”‚\n",
    "                     â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  llama.cpp Server                               â”‚\n",
    "â”‚  http://127.0.0.1:8080/v1/chat/completions      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "## Features\n",
    "\n",
    "âœ… **Direct llama.cpp integration** - No FastAPI overhead  \n",
    "âœ… **Optional context support** - Pass context_chunks or None  \n",
    "âœ… **Async/sync wrappers** - Easy to use in notebooks  \n",
    "âœ… **Parallel processing** - Process multiple questions concurrently  \n",
    "âœ… **Flexible modes** - Choose between RAG API or direct generation\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "1. **Make sure llama.cpp server is running:**\n",
    "   ```bash\n",
    "   # Check if server is running\n",
    "   curl http://127.0.0.1:8080/health\n",
    "   ```\n",
    "\n",
    "2. **Run cell 1** to initialize `OllamaCppClient`\n",
    "\n",
    "3. **Choose your mode:**\n",
    "   - **Direct generation (fastest):** Uses current `main()` function\n",
    "   - **With RAG API:** Use `ask_single_question_with_rag()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1360c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 58 PDF files:\n",
      "Llama.cpp client initialized at: http://127.0.0.1:8080\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['data\\\\pdf\\\\Public_427.pdf',\n",
       " 'data\\\\pdf\\\\Public_428.pdf',\n",
       " 'data\\\\pdf\\\\Public_441.pdf',\n",
       " 'data\\\\pdf\\\\Public_442.pdf',\n",
       " 'data\\\\pdf\\\\Public_444.pdf',\n",
       " 'data\\\\pdf\\\\Public_445.pdf',\n",
       " 'data\\\\pdf\\\\Public_446.pdf',\n",
       " 'data\\\\pdf\\\\Public_447.pdf',\n",
       " 'data\\\\pdf\\\\Public_448.pdf',\n",
       " 'data\\\\pdf\\\\Public_449.pdf',\n",
       " 'data\\\\pdf\\\\Public_465.pdf',\n",
       " 'data\\\\pdf\\\\Public_466.pdf',\n",
       " 'data\\\\pdf\\\\Public_468.pdf',\n",
       " 'data\\\\pdf\\\\Public_469.pdf',\n",
       " 'data\\\\pdf\\\\Public_470.pdf',\n",
       " 'data\\\\pdf\\\\Public_471.pdf',\n",
       " 'data\\\\pdf\\\\Public_472.pdf',\n",
       " 'data\\\\pdf\\\\Public_473.pdf',\n",
       " 'data\\\\pdf\\\\Public_474.pdf',\n",
       " 'data\\\\pdf\\\\Public_624.pdf',\n",
       " 'data\\\\pdf\\\\Public_626.pdf',\n",
       " 'data\\\\pdf\\\\Public_634.pdf',\n",
       " 'data\\\\pdf\\\\Public_635.pdf',\n",
       " 'data\\\\pdf\\\\Public_636.pdf',\n",
       " 'data\\\\pdf\\\\Public_637.pdf',\n",
       " 'data\\\\pdf\\\\Public_638.pdf',\n",
       " 'data\\\\pdf\\\\Public_639.pdf',\n",
       " 'data\\\\pdf\\\\Public_640.pdf',\n",
       " 'data\\\\pdf\\\\Public_641.pdf',\n",
       " 'data\\\\pdf\\\\Public_642.pdf',\n",
       " 'data\\\\pdf\\\\Public_643.pdf',\n",
       " 'data\\\\pdf\\\\Public_644.pdf',\n",
       " 'data\\\\pdf\\\\Public_645.pdf',\n",
       " 'data\\\\pdf\\\\Public_646.pdf',\n",
       " 'data\\\\pdf\\\\Public_647.pdf',\n",
       " 'data\\\\pdf\\\\Public_648.pdf',\n",
       " 'data\\\\pdf\\\\Public_649.pdf',\n",
       " 'data\\\\pdf\\\\Public_650.pdf',\n",
       " 'data\\\\pdf\\\\Public_651.pdf',\n",
       " 'data\\\\pdf\\\\Public_652.pdf',\n",
       " 'data\\\\pdf\\\\Public_653.pdf',\n",
       " 'data\\\\pdf\\\\Public_654.pdf',\n",
       " 'data\\\\pdf\\\\Public_655.pdf',\n",
       " 'data\\\\pdf\\\\Public_656.pdf',\n",
       " 'data\\\\pdf\\\\Public_657.pdf',\n",
       " 'data\\\\pdf\\\\Public_658.pdf',\n",
       " 'data\\\\pdf\\\\Public_659.pdf',\n",
       " 'data\\\\pdf\\\\Public_660.pdf',\n",
       " 'data\\\\pdf\\\\Public_661.pdf',\n",
       " 'data\\\\pdf\\\\Public_662.pdf',\n",
       " 'data\\\\pdf\\\\Public_665.pdf',\n",
       " 'data\\\\pdf\\\\Public_668.pdf',\n",
       " 'data\\\\pdf\\\\Public_669.pdf',\n",
       " 'data\\\\pdf\\\\Public_670.pdf',\n",
       " 'data\\\\pdf\\\\Public_671.pdf',\n",
       " 'data\\\\pdf\\\\Public_672.pdf',\n",
       " 'data\\\\pdf\\\\Public_673.pdf',\n",
       " 'data\\\\pdf\\\\Public_675.pdf']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from src.config import get_settings\n",
    "from src.services.llamacpp.llamacpp_client import OllamaCppClient\n",
    "\n",
    "# ./server -m \"C:\\Users\\APC\\AppData\\Local\\llama.cpp\\bartowski_SmallThinker-3B-Preview-GGUF_SmallThinker-3B-Preview-Q4_K_M.gguf\" -c 16384\n",
    "API_BASE_URL = \"http://localhost:8000\"\n",
    "AIRFLOW_URL = \"http://localhost:8080\"\n",
    "\n",
    "# Host & port cÃ¡c service\n",
    "POSTGRES_HOST = \"localhost\"\n",
    "POSTGRES_PORT = 5432          # hoáº·c 5433 náº¿u muá»‘n connect airflow-db\n",
    "REDIS_HOST = \"localhost\"\n",
    "REDIS_PORT = 6379\n",
    "OPENSEARCH_HOST = \"http://localhost:9200\"\n",
    "LLAMACPP_HOST = \"http://127.0.0.1:8080\"\n",
    "\n",
    "# Cáº¥u hÃ¬nh RAG\n",
    "TOP_K = 10\n",
    "USE_HYBRID = True\n",
    "TIMEOUT = 180\n",
    "MAX_WORKERS = 2\n",
    "\n",
    "# Initialize llama.cpp client\n",
    "llama_client = OllamaCppClient(\n",
    "    host=LLAMACPP_HOST,\n",
    "    model=\"gpt-like-model\",\n",
    "    temperature=0.3,\n",
    "    max_response_words=300,\n",
    "    timeout=TIMEOUT\n",
    ")\n",
    "\n",
    "settings = get_settings()\n",
    "pdf_dir = Path(settings.data.pdf_dir)\n",
    "\n",
    "pdf_files = list(pdf_dir.glob(\"*.pdf\"))\n",
    "pdf_paths = [str(f) for f in pdf_files]\n",
    "\n",
    "print(f\"Found {len(pdf_paths)} PDF files:\")\n",
    "print(f\"Llama.cpp client initialized at: {LLAMACPP_HOST}\")\n",
    "pdf_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f095d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List\n",
    "from datetime import datetime\n",
    "import socket\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# ÄÆ°á»ng dáº«n files\n",
    "DATA_DIR = Path(\"data\")\n",
    "PDF_DIR = DATA_DIR / \"pdf\"\n",
    "QUESTION_FILE = DATA_DIR / \"question.csv\"\n",
    "OUTPUT_FILE = DATA_DIR / \"answers_mcq.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d092690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“Œ BÆ¯á»šC 4: Äá»c cÃ¢u há»i tá»« CSV\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "âœ… Äá»c thÃ nh cÃ´ng 282 cÃ¢u há»i\n",
      "\n",
      "ğŸ“ VÃ­ dá»¥ cÃ¢u há»i Ä‘áº§u tiÃªn:\n",
      "   Q: Tuá»•i thá» trung bÃ¬nh cá»§a táº¥m pin nÄƒng lÆ°á»£ng máº·t trá»i lÃ  bao nhiÃªu nÄƒm?...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tuá»•i thá» trung bÃ¬nh cá»§a táº¥m pin nÄƒng lÆ°á»£ng máº·t...</td>\n",
       "      <td>20 â€“ 25 nÄƒm</td>\n",
       "      <td>10 â€“ 12 nÄƒm</td>\n",
       "      <td>5 â€“ 7 nÄƒm</td>\n",
       "      <td>10 - 15 nÄƒm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Biá»ƒu tÆ°á»£ng nÃ o hiá»ƒn thá»‹ mÃ u lá»¥c khi Ä‘á»™ng cÆ¡ se...</td>\n",
       "      <td>Biá»ƒu tÆ°á»£ng cháº¿ Ä‘á»™ thá»§ cÃ´ng</td>\n",
       "      <td>Biá»ƒu tÆ°á»£ng gia nhiá»‡t khoang chá»©a</td>\n",
       "      <td>Biá»ƒu tÆ°á»£ng hoáº¡t Ä‘á»™ng cá»§a Ä‘á»™ng cÆ¡ servo</td>\n",
       "      <td>Biá»ƒu tÆ°á»£ng bÃ¡o Ä‘á»™ng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Theo tÃ i liá»‡u Public_656, LED nÃ o sÃ¡ng khi trá»‘...</td>\n",
       "      <td>Toner LED</td>\n",
       "      <td>Drum LED</td>\n",
       "      <td>Error LED</td>\n",
       "      <td>Ready LED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NÃºt bÃ n di chuá»™t pháº£i cÃ³ chá»©c nÄƒng gÃ¬?</td>\n",
       "      <td>Hoáº¡t Ä‘á»™ng nhÆ° nÃºt trÃ¡i chuá»™t ngoÃ i</td>\n",
       "      <td>Hoáº¡t Ä‘á»™ng nhÆ° nÃºt pháº£i chuá»™t ngoÃ i</td>\n",
       "      <td>DÃ¹ng Ä‘á»ƒ cuá»™n trang</td>\n",
       "      <td>DÃ¹ng Ä‘á»ƒ báº­t/táº¯t touchpad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ThÃ nh pháº§n nÃ o dÃ¹ng Ä‘á»ƒ Ä‘áº·t giá»›i háº¡n tá»‘i Ä‘a cÃ³ ...</td>\n",
       "      <td>Manual Settings Limits</td>\n",
       "      <td>Auto Settings Limits</td>\n",
       "      <td>Mechanical Settings Limits</td>\n",
       "      <td>Servo Movement Alarms</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0  Tuá»•i thá» trung bÃ¬nh cá»§a táº¥m pin nÄƒng lÆ°á»£ng máº·t...   \n",
       "1  Biá»ƒu tÆ°á»£ng nÃ o hiá»ƒn thá»‹ mÃ u lá»¥c khi Ä‘á»™ng cÆ¡ se...   \n",
       "2  Theo tÃ i liá»‡u Public_656, LED nÃ o sÃ¡ng khi trá»‘...   \n",
       "3             NÃºt bÃ n di chuá»™t pháº£i cÃ³ chá»©c nÄƒng gÃ¬?   \n",
       "4  ThÃ nh pháº§n nÃ o dÃ¹ng Ä‘á»ƒ Ä‘áº·t giá»›i háº¡n tá»‘i Ä‘a cÃ³ ...   \n",
       "\n",
       "                                    A                                   B  \\\n",
       "0                         20 â€“ 25 nÄƒm                         10 â€“ 12 nÄƒm   \n",
       "1          Biá»ƒu tÆ°á»£ng cháº¿ Ä‘á»™ thá»§ cÃ´ng    Biá»ƒu tÆ°á»£ng gia nhiá»‡t khoang chá»©a   \n",
       "2                           Toner LED                            Drum LED   \n",
       "3  Hoáº¡t Ä‘á»™ng nhÆ° nÃºt trÃ¡i chuá»™t ngoÃ i  Hoáº¡t Ä‘á»™ng nhÆ° nÃºt pháº£i chuá»™t ngoÃ i   \n",
       "4              Manual Settings Limits                Auto Settings Limits   \n",
       "\n",
       "                                        C                         D  \n",
       "0                               5 â€“ 7 nÄƒm               10 - 15 nÄƒm  \n",
       "1  Biá»ƒu tÆ°á»£ng hoáº¡t Ä‘á»™ng cá»§a Ä‘á»™ng cÆ¡ servo       Biá»ƒu tÆ°á»£ng bÃ¡o Ä‘á»™ng  \n",
       "2                               Error LED                 Ready LED  \n",
       "3                      DÃ¹ng Ä‘á»ƒ cuá»™n trang  DÃ¹ng Ä‘á»ƒ báº­t/táº¯t touchpad  \n",
       "4              Mechanical Settings Limits     Servo Movement Alarms  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def print_header(text: str):\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"  {text}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "def print_step(step_num: int, text: str):\n",
    "    print(f\"\\n{'â”€' * 80}\")\n",
    "    print(f\"ğŸ“Œ BÆ¯á»šC {step_num}: {text}\")\n",
    "    print(f\"{'â”€' * 80}\")\n",
    "\n",
    "def print_step(step_num: int, text: str):\n",
    "    print(f\"\\n{'â”€' * 80}\")\n",
    "    print(f\"ğŸ“Œ BÆ¯á»šC {step_num}: {text}\")\n",
    "    print(f\"{'â”€' * 80}\")\n",
    "\n",
    "\n",
    "def load_questions() -> pd.DataFrame:\n",
    "    \"\"\"Äá»c cÃ¢u há»i tá»« CSV file.\"\"\"\n",
    "    print_step(4, \"Äá»c cÃ¢u há»i tá»« CSV\")\n",
    "\n",
    "    if not QUESTION_FILE.exists():\n",
    "        print(f\"\\nâŒ File khÃ´ng tá»“n táº¡i: {QUESTION_FILE}\")\n",
    "        print(\"Vui lÃ²ng táº¡o file question.csv vá»›i format:\")\n",
    "        print(\"  Question,A,B,C,D,source_folder\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(QUESTION_FILE)\n",
    "        print(f\"\\nâœ… Äá»c thÃ nh cÃ´ng {len(df)} cÃ¢u há»i\")\n",
    "\n",
    "        # Kiá»ƒm tra columns\n",
    "        required_cols = [\"Question\", \"A\", \"B\", \"C\", \"D\"]\n",
    "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "\n",
    "        if missing_cols:\n",
    "            print(f\"âŒ Thiáº¿u columns: {missing_cols}\")\n",
    "            return None\n",
    "\n",
    "        # Hiá»ƒn thá»‹ sample\n",
    "        print(\"\\nğŸ“ VÃ­ dá»¥ cÃ¢u há»i Ä‘áº§u tiÃªn:\")\n",
    "        print(f\"   Q: {df.iloc[0]['Question']}...\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Lá»—i khi Ä‘á»c CSV: {e}\")\n",
    "        return None\n",
    "\n",
    "load_questions().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61c84ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Use RAG API endpoint (retrieval + generation)\n",
    "# def ask_single_question_with_rag(\n",
    "#     question: str, options: Dict[str, str], source_folder: str = None\n",
    "# ) -> Dict[str, Any]:\n",
    "#     \"\"\"Tráº£ lá»i cÃ¢u há»i MCQ using full RAG API (hybrid retrieval + llama.cpp generation).\"\"\"\n",
    "#     payload = {\"question\": question, \"options\": options, \"top_k\": TOP_K, \"use_hybrid\": USE_HYBRID}\n",
    "#     if source_folder and pd.notna(source_folder):\n",
    "#         payload[\"source_folder\"] = source_folder\n",
    "#     try:\n",
    "#         response = requests.post(f\"{API_BASE_URL}/api/v1/ask\", json=payload, timeout=TIMEOUT)\n",
    "#         response.raise_for_status()\n",
    "#         return response.json()\n",
    "#     except requests.exceptions.Timeout:\n",
    "#         print(f\"    â±ï¸  Timeout (>{TIMEOUT}s)\")\n",
    "#         return {\"error\": \"timeout\", \"predicted_option\": None}\n",
    "#     except Exception as e:\n",
    "#         print(f\"    âŒ Lá»—i: {str(e)[:100]}\")\n",
    "#         return {\"error\": str(e), \"predicted_option\": None}\n",
    "\n",
    "\n",
    "# Option 2: Use OllamaCppClient directly (manual retrieval, just generation)\n",
    "async def ask_single_question_direct(\n",
    "    question: str,\n",
    "    options: Dict[str, str],\n",
    "    context_chunks: List[Dict] = None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Tráº£ lá»i cÃ¢u há»i MCQ using OllamaCppClient directly.\n",
    "\n",
    "    Args:\n",
    "        question: Question text\n",
    "        options: Dict of options {\"A\": \"...\", \"B\": \"...\", ...}\n",
    "        context_chunks: Optional list of retrieved context chunks\n",
    "                        If None, will answer without context\n",
    "\n",
    "    Returns:\n",
    "        Dict with predicted_option, reasoning, confidence, etc.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use empty context if none provided\n",
    "        if context_chunks is None:\n",
    "            context_chunks = []\n",
    "\n",
    "        # Call llama.cpp client directly\n",
    "        result = await llama_client.generate_mcq_answer(\n",
    "            question=question,\n",
    "            options=options,\n",
    "            context_chunks=context_chunks\n",
    "        )\n",
    "\n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"    âŒ Lá»—i: {str(e)[:100]}\")\n",
    "        return {\n",
    "            \"error\": str(e),\n",
    "            \"predicted_option\": None,\n",
    "            \"confidence\": \"low\",\n",
    "            \"reasoning\": f\"Error: {str(e)}\"\n",
    "        }\n",
    "\n",
    "\n",
    "# Option 3: Synchronous wrapper for direct client usage\n",
    "def ask_single_question_cpp(\n",
    "    question: str,\n",
    "    options: Dict[str, str],\n",
    "    source_folder: str = None,\n",
    "    context_chunks: List[Dict] = None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Synchronous wrapper for OllamaCppClient.\n",
    "    Can be used with or without context chunks.\n",
    "    \"\"\"\n",
    "    import asyncio\n",
    "\n",
    "    # Create event loop if needed\n",
    "    try:\n",
    "        loop = asyncio.get_event_loop()\n",
    "    except RuntimeError:\n",
    "        loop = asyncio.new_event_loop()\n",
    "        asyncio.set_event_loop(loop)\n",
    "\n",
    "    # Run async function\n",
    "    result = loop.run_until_complete(\n",
    "        ask_single_question_direct(question, options, context_chunks)\n",
    "    )\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35455613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from IPython.display import Markdown\n",
    "\n",
    "def retrieve(query, top_k=TOP_K, use_hybrid=USE_HYBRID):\n",
    "    \"\"\"Retrieve chunks from OpenSearch API.\"\"\"\n",
    "    url = \"http://localhost:8000/api/v1/search/\"  # Fixed URL (was missing /)\n",
    "    payload = {\n",
    "        \"query\": query,\n",
    "        \"top_k\": top_k,\n",
    "        \"use_hybrid\": use_hybrid\n",
    "    }\n",
    "    try:\n",
    "        r = requests.post(url, json=payload, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        return r.json()\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"âŒ retrieve() error: {e}\")\n",
    "        return {\"hits\": []}\n",
    "\n",
    "\n",
    "def extract_doc_id_from_question(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract Public_XXX pattern from question text.\n",
    "    Returns the pattern if found, otherwise None.\n",
    "\n",
    "    Example: \"Question about Public_422\" -> \"Public_422\"\n",
    "    \"\"\"\n",
    "    match = re.search(r'Public_\\d+', question)\n",
    "    return match.group(0) if match else None\n",
    "\n",
    "\n",
    "def dynamic_retrieve_context(question: str, top_k: int = TOP_K) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Retrieve top_k chunks for question.\n",
    "    Auto-adjusts top_k and filtering based on question pattern.\n",
    "\n",
    "    - If Public_XXX pattern found: top_k=30 and filter by doc_id\n",
    "    - Otherwise: use default top_k\n",
    "\n",
    "    Returns list of chunk dicts with metadata.\n",
    "    \"\"\"\n",
    "    # Try to find Public_XXX pattern in question first\n",
    "    doc_id_prefix = extract_doc_id_from_question(question)\n",
    "\n",
    "    # âœ… Dynamic top_k adjustment\n",
    "    effective_top_k = 30 if doc_id_prefix else top_k\n",
    "\n",
    "    # Get chunks from OpenSearch with adjusted top_k\n",
    "    response_data = retrieve(question, top_k=effective_top_k, use_hybrid=USE_HYBRID)\n",
    "    hits = response_data.get('hits', [])\n",
    "\n",
    "    results = []\n",
    "    for hit in hits:\n",
    "        chunk_id = hit.get('chunk_id', '')\n",
    "\n",
    "        # Filter by doc_id_prefix only if found in question\n",
    "        if doc_id_prefix:\n",
    "            if not chunk_id.startswith(doc_id_prefix):\n",
    "                continue  # Skip chunks that don't match doc_id\n",
    "\n",
    "        # Build chunk dict with metadata\n",
    "        results.append({\n",
    "            'chunk_id': chunk_id,\n",
    "            'chunk_text': hit.get('chunk_text', ''),\n",
    "            'section_name': hit.get('section_name'),\n",
    "            'document_id': hit.get('document_id'),\n",
    "            'document_file_name': hit.get('document_file_name'),\n",
    "            'document_title': hit.get('document_title'),\n",
    "        })\n",
    "\n",
    "    # âœ… Better logging\n",
    "    if doc_id_prefix:\n",
    "        print(f\"âœ… Found {len(results)} chunks (filtered by {doc_id_prefix}, searched top {effective_top_k})\")\n",
    "    else:\n",
    "        print(f\"âœ… Found {len(results)} chunks (default search with top {effective_top_k})\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65753840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_single_question(args: tuple) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Process a single question for parallel execution.\n",
    "    Uses OllamaCppClient directly without context retrieval.\n",
    "\n",
    "    Args:\n",
    "        args: Tuple of (index, row, q_num, total)\n",
    "\n",
    "    Returns:\n",
    "        Result dictionary for the question\n",
    "    \"\"\"\n",
    "    idx, row, q_num, total = args\n",
    "\n",
    "    progress = (q_num / total) * 100\n",
    "    print(f\"[{q_num}/{total}] ({progress:.1f}%) Q: {row['Question']}\")\n",
    "\n",
    "    options = {\"A\": str(row[\"A\"]), \"B\": str(row[\"B\"]), \"C\": str(row[\"C\"]), \"D\": str(row[\"D\"])}\n",
    "    source_folder = row.get(\"source_folder\", None)\n",
    "\n",
    "    q_start = time.time()\n",
    "\n",
    "    # Use direct llama.cpp client (no context retrieval)\n",
    "    # If you want to add context, retrieve it first and pass to context_chunks parameter\n",
    "    answer_data = ask_single_question_cpp(\n",
    "        question=row[\"Question\"],\n",
    "        options=options,\n",
    "        context_chunks=dynamic_retrieve_context(row[\"Question\"])  # No context - direct generation only\n",
    "        # context_chunks=[]  # No context - direct generation only\n",
    "    )\n",
    "\n",
    "    q_time = time.time() - q_start\n",
    "\n",
    "    predicted = answer_data.get(\"predicted_option\", \"N/A\")\n",
    "    confidence = answer_data.get(\"confidence\", \"unknown\")\n",
    "    reasoning = answer_data.get(\"reasoning\", \"\")\n",
    "    error = answer_data.get(\"error\", None)\n",
    "\n",
    "    conf_icon = {\"high\": \"ğŸŸ¢\", \"medium\": \"ğŸŸ¡\", \"low\": \"ğŸ”´\"}.get(confidence, \"âšª\")\n",
    "    print(f\"    â†’ ÄÃ¡p Ã¡n: {predicted} {conf_icon} ({q_time:.1f}s)\")\n",
    "    print(f\"      Suy luáº­n: {reasoning}\")\n",
    "\n",
    "    result = {\n",
    "        \"question_number\": q_num,\n",
    "        \"question\": row[\"Question\"],\n",
    "        \"option_A\": row[\"A\"],\n",
    "        \"option_B\": row[\"B\"],\n",
    "        \"option_C\": row[\"C\"],\n",
    "        \"option_D\": row[\"D\"],\n",
    "        \"source_folder\": source_folder if pd.notna(source_folder) else \"\",\n",
    "        \"predicted_answer\": predicted,\n",
    "        \"confidence\": confidence,\n",
    "        \"reasoning\": reasoning[:500] if reasoning else \"\",\n",
    "        \"processing_time_seconds\": round(q_time, 2),\n",
    "        \"error\": error if error else \"\",\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "    }\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def answer_all_questions(df: pd.DataFrame, max_workers: int = 4) -> pd.DataFrame:\n",
    "    \"\"\"Tráº£ lá»i táº¥t cáº£ cÃ¢u há»i trong dataframe sá»­ dá»¥ng parallel processing.\"\"\"\n",
    "    from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "    print_step(5, \"Tráº£ lá»i cÃ¢u há»i (Parallel)\")\n",
    "\n",
    "    results = []\n",
    "    total = len(df)\n",
    "\n",
    "    print(f\"\\nğŸ¯ Báº¯t Ä‘áº§u tráº£ lá»i {total} cÃ¢u há»i...\\n\")\n",
    "    print(f\"âš™ï¸  Using {max_workers} parallel workers\\n\")\n",
    "    # print(f\"ğŸ”§ Mode: Direct llama.cpp generation (no context retrieval)\\n\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Prepare task arguments\n",
    "    tasks = [(idx, row, idx + 1, total) for idx, (_, row) in enumerate(df.iterrows())]\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(_process_single_question, task): task[0] for task in tasks}\n",
    "\n",
    "        completed_count = 0\n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                result = future.result(timeout=TIMEOUT + 30)\n",
    "                results.append(result)\n",
    "                completed_count += 1\n",
    "\n",
    "                if completed_count % 5 == 0:\n",
    "                    print()\n",
    "\n",
    "            except Exception as e:\n",
    "                completed_count += 1\n",
    "                print(f\"âŒ Error processing question: {str(e)[:100]}\")\n",
    "                results.append({\n",
    "                    \"error\": str(e),\n",
    "                    \"predicted_option\": None,\n",
    "                    \"question_number\": futures[future] + 1\n",
    "                })\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "    print(\"\\n\" + \"â”€\" * 80)\n",
    "    print(f\"âœ… HoÃ n thÃ nh! Tá»•ng thá»i gian: {total_time:.1f}s\")\n",
    "    print(f\"â±ï¸  Trung bÃ¬nh: {total_time / total:.1f}s/cÃ¢u\")\n",
    "    print(f\"âš¡ Tá»‘c Ä‘á»™: {total / total_time:.2f} cÃ¢u/giÃ¢y\")\n",
    "    print(\"â”€\" * 80)\n",
    "\n",
    "    # Sort results by question number\n",
    "    results.sort(key=lambda x: x.get(\"question_number\", 0))\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def save_results(results_df: pd.DataFrame):\n",
    "    \"\"\"LÆ°u káº¿t quáº£ ra CSV file.\"\"\"\n",
    "    print_step(6, \"LÆ°u káº¿t quáº£\")\n",
    "\n",
    "    try:\n",
    "        results_df.to_csv(OUTPUT_FILE, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "        print(f\"\\nâœ… ÄÃ£ lÆ°u káº¿t quáº£ vÃ o: {OUTPUT_FILE}\")\n",
    "        print(f\"ğŸ“Š Tá»•ng sá»‘ cÃ¢u: {len(results_df)}\")\n",
    "\n",
    "        # Thá»‘ng kÃª\n",
    "        conf_counts = results_df[\"confidence\"].value_counts()\n",
    "        print(\"\\nğŸ“ˆ Thá»‘ng kÃª Ä‘á»™ tin cáº­y:\")\n",
    "        for conf, count in conf_counts.items():\n",
    "            percentage = (count / len(results_df)) * 100\n",
    "            print(f\"  â€¢ {conf}: {count} cÃ¢u ({percentage:.1f}%)\")\n",
    "\n",
    "        # Kiá»ƒm tra errors\n",
    "        errors = results_df[results_df[\"error\"] != \"\"]\n",
    "        if len(errors) > 0:\n",
    "            print(f\"\\nâš ï¸  CÃ³ {len(errors)} cÃ¢u bá»‹ lá»—i:\")\n",
    "            for idx, row in errors.iterrows():\n",
    "                print(f\"  â€¢ CÃ¢u {row['question_number']}: {row['error']}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Lá»—i khi lÆ°u file: {e}\")\n",
    "\n",
    "def display_summary(results_df: pd.DataFrame):\n",
    "    \"\"\"Hiá»ƒn thá»‹ tÃ³m táº¯t káº¿t quáº£.\"\"\"\n",
    "    print_header(\"ğŸ“Š TÃ“M Táº®T Káº¾T QUáº¢\")\n",
    "\n",
    "    total = len(results_df)\n",
    "    successful = len(results_df[results_df[\"error\"] == \"\"])\n",
    "\n",
    "    print(f\"\\nâœ… Tá»•ng sá»‘ cÃ¢u: {total}\")\n",
    "    print(f\"âœ… Tráº£ lá»i thÃ nh cÃ´ng: {successful}\")\n",
    "    print(f\"âŒ Lá»—i: {total - successful}\")\n",
    "\n",
    "    # Top 5 cÃ¢u cÃ³ confidence cao\n",
    "    high_conf = results_df[results_df[\"confidence\"] == \"high\"].head(20)\n",
    "    if len(high_conf) > 0:\n",
    "        print(f\"\\nğŸŸ¢ Top {len(high_conf)} cÃ¢u cÃ³ Ä‘á»™ tin cáº­y cao:\")\n",
    "        for idx, row in high_conf.iterrows():\n",
    "            print(\n",
    "                f\"  {row['question_number']}. ÄÃ¡p Ã¡n {row['predicted_answer']}: {row['question']}\"\n",
    "            )\n",
    "\n",
    "    # CÃ¢u cÃ³ confidence tháº¥p\n",
    "    low_conf = results_df[results_df[\"confidence\"] == \"low\"]\n",
    "    if len(low_conf) > 0:\n",
    "        print(f\"\\nğŸ”´ {len(low_conf)} cÃ¢u cÃ³ Ä‘á»™ tin cáº­y tháº¥p:\")\n",
    "        for idx, row in low_conf.head(20).iterrows():\n",
    "            print(f\"  {row['question_number']}. {row['question']}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f97d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  ğŸš€ RAG MCQ SYSTEM - AUTOMATIC ANSWERING\n",
      "================================================================================\n",
      "\n",
      "ğŸ“… Báº¯t Ä‘áº§u: 2025-11-12 08:31:19\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“Œ BÆ¯á»šC 4: Äá»c cÃ¢u há»i tá»« CSV\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "âœ… Äá»c thÃ nh cÃ´ng 282 cÃ¢u há»i\n",
      "\n",
      "ğŸ“ VÃ­ dá»¥ cÃ¢u há»i Ä‘áº§u tiÃªn:\n",
      "   Q: Tuá»•i thá» trung bÃ¬nh cá»§a táº¥m pin nÄƒng lÆ°á»£ng máº·t trá»i lÃ  bao nhiÃªu nÄƒm?...\n",
      "                                              Question  \\\n",
      "0    Tuá»•i thá» trung bÃ¬nh cá»§a táº¥m pin nÄƒng lÆ°á»£ng máº·t...   \n",
      "1    Biá»ƒu tÆ°á»£ng nÃ o hiá»ƒn thá»‹ mÃ u lá»¥c khi Ä‘á»™ng cÆ¡ se...   \n",
      "2    Theo tÃ i liá»‡u Public_656, LED nÃ o sÃ¡ng khi trá»‘...   \n",
      "3               NÃºt bÃ n di chuá»™t pháº£i cÃ³ chá»©c nÄƒng gÃ¬?   \n",
      "4    ThÃ nh pháº§n nÃ o dÃ¹ng Ä‘á»ƒ Ä‘áº·t giá»›i háº¡n tá»‘i Ä‘a cÃ³ ...   \n",
      "..                                                 ...   \n",
      "277  Trong TD635, cÃ³ tá»•ng cá»™ng bao nhiÃªu loáº¡i Ä‘áº§u d...   \n",
      "278  Náº¿u má»™t thá»£ kim hoÃ n cáº§n táº¡o khuÃ´n vÃ  thiáº¿t káº¿...   \n",
      "279  â€œBu lÃ´ngâ€ vÃ  â€œCuplerâ€ trong TD647 cÃ³ TiÃªu chuáº©...   \n",
      "280  Vá»›i hÃ³a Ä‘Æ¡n Ä‘iá»‡n hÃ ng thÃ¡ng khoáº£ng 3 triá»‡u Ä‘á»“n...   \n",
      "281  Trong TD639, tiÃªu chuáº©n ká»¹ thuáº­t Ã¡p dá»¥ng cho Ä‘...   \n",
      "\n",
      "                                                     A  \\\n",
      "0                                          20 â€“ 25 nÄƒm   \n",
      "1                           Biá»ƒu tÆ°á»£ng cháº¿ Ä‘á»™ thá»§ cÃ´ng   \n",
      "2                                            Toner LED   \n",
      "3                   Hoáº¡t Ä‘á»™ng nhÆ° nÃºt trÃ¡i chuá»™t ngoÃ i   \n",
      "4                               Manual Settings Limits   \n",
      "..                                                 ...   \n",
      "277                                                  3   \n",
      "278  CÃ³, vÃ¬ mÃ¡y cho Ä‘á»™ chi tiáº¿t cá»±c cao vÃ  phÃ¹ há»£p ...   \n",
      "279                  TCVN 197-1:2014 vÃ  TCVN 1651:2018   \n",
      "280          Há»‡ Ä‘iá»‡n máº·t trá»i hÃ²a lÆ°á»›i cÃ´ng suáº¥t 5 kWp   \n",
      "281                                  TCVN 7722-1:2017    \n",
      "\n",
      "                                      B  \\\n",
      "0                           10 â€“ 12 nÄƒm   \n",
      "1      Biá»ƒu tÆ°á»£ng gia nhiá»‡t khoang chá»©a   \n",
      "2                              Drum LED   \n",
      "3    Hoáº¡t Ä‘á»™ng nhÆ° nÃºt pháº£i chuá»™t ngoÃ i   \n",
      "4                  Auto Settings Limits   \n",
      "..                                  ...   \n",
      "277                                   4   \n",
      "278     KhÃ´ng, vÃ¬ chá»‰ dÃ¹ng cho giÃ¡o dá»¥c   \n",
      "279               ASTM A370 vÃ  ASTM A53   \n",
      "280              Há»‡ Ä‘iá»‡n máº·t trá»i 3 kWp   \n",
      "281                      TCVN 4453:1995   \n",
      "\n",
      "                                                C  \\\n",
      "0                                       5 â€“ 7 nÄƒm   \n",
      "1          Biá»ƒu tÆ°á»£ng hoáº¡t Ä‘á»™ng cá»§a Ä‘á»™ng cÆ¡ servo   \n",
      "2                                       Error LED   \n",
      "3                              DÃ¹ng Ä‘á»ƒ cuá»™n trang   \n",
      "4                      Mechanical Settings Limits   \n",
      "..                                            ...   \n",
      "277                                             5   \n",
      "278  KhÃ´ng, vÃ¬ chá»‰ in Ä‘Æ°á»£c mÃ´ hÃ¬nh kÃ­ch thÆ°á»›c lá»›n   \n",
      "279             TCVN 1916:1995 vÃ  TCVN 8163:2009    \n",
      "280                        Há»‡ Ä‘iá»‡n máº·t trá»i 1 kWp   \n",
      "281                                 ISO 9002:2005   \n",
      "\n",
      "                                            D  \n",
      "0                                 10 - 15 nÄƒm  \n",
      "1                         Biá»ƒu tÆ°á»£ng bÃ¡o Ä‘á»™ng  \n",
      "2                                   Ready LED  \n",
      "3                    DÃ¹ng Ä‘á»ƒ báº­t/táº¯t touchpad  \n",
      "4                       Servo Movement Alarms  \n",
      "..                                        ...  \n",
      "277                                         6  \n",
      "278  CÃ³, vÃ¬ phÃ¹ há»£p vá»›i cÃ¡c sáº£n pháº©m Ä‘Æ¡n giáº£n  \n",
      "279      ISO 15630-1:2010 vÃ  TCVN 7937-1:2013  \n",
      "280                          Cáº£ 3 Ä‘Ã¡p Ã¡n trÃªn  \n",
      "281                            TCVN 1234:2010  \n",
      "\n",
      "[282 rows x 5 columns]\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“Œ BÆ¯á»šC 5: Tráº£ lá»i cÃ¢u há»i (Parallel)\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ¯ Báº¯t Ä‘áº§u tráº£ lá»i 282 cÃ¢u há»i...\n",
      "\n",
      "âš™ï¸  Using 4 parallel workers\n",
      "\n",
      "[1/282] (0.4%) Q: Tuá»•i thá» trung bÃ¬nh cá»§a táº¥m pin nÄƒng lÆ°á»£ng máº·t trá»i lÃ  bao nhiÃªu nÄƒm?\n",
      "[2/282] (0.7%) Q: Biá»ƒu tÆ°á»£ng nÃ o hiá»ƒn thá»‹ mÃ u lá»¥c khi Ä‘á»™ng cÆ¡ servo Ä‘ang báº­t?\n",
      "[3/282] (1.1%) Q: Theo tÃ i liá»‡u Public_656, LED nÃ o sÃ¡ng khi trá»‘ng (Drum) cáº§n Ä‘Æ°á»£c thay tháº¿ sá»›m?\n",
      "[4/282] (1.4%) Q: NÃºt bÃ n di chuá»™t pháº£i cÃ³ chá»©c nÄƒng gÃ¬?\n",
      "âœ… Found 20 chunks (default search with top 20)\n",
      "Prompt TOKEN COUNT: 12287\n",
      "âœ… Found 5 chunks (filtered by Public_656, searched top 30)\n",
      "Prompt TOKEN COUNT: 2558\n",
      "âœ… Found 20 chunks (default search with top 20)\n",
      "Prompt TOKEN COUNT: 10138\n",
      "âœ… Found 20 chunks (default search with top 20)\n",
      "Prompt TOKEN COUNT: 13923\n",
      "    â†’ ÄÃ¡p Ã¡n: B ğŸŸ¢ (47.1s)\n",
      "      Suy luáº­n: TÃ i liá»‡u 2 (Public_655.pdf) nÃªu rÃµ \"NÃºt bÃ n di chuá»™t pháº£i\" hoáº¡t Ä‘á»™ng nhÆ° nÃºt pháº£i cá»§a chuá»™t ngoÃ i.\n",
      "[5/282] (1.8%) Q: ThÃ nh pháº§n nÃ o dÃ¹ng Ä‘á»ƒ Ä‘áº·t giá»›i háº¡n tá»‘i Ä‘a cÃ³ thá»ƒ Ä‘iá»u chá»‰nh khi á»Ÿ cháº¿ Ä‘á»™ thá»§ cÃ´ng cá»§a E-Drive?\n",
      "âœ… Found 20 chunks (default search with top 20)\n",
      "Prompt TOKEN COUNT: 12043\n",
      "    â†’ ÄÃ¡p Ã¡n: B ğŸŸ¢ (61.0s)\n",
      "      Suy luáº­n: The document states \"Drum LED: biá»ƒu thá»‹ Drum cáº§n Ä‘Æ°á»£c thay tháº¿ sá»›m\" (Drum LED indicates the drum needs to be replaced soon). The \"DRUMENDSOON\" entry in the table also confirms this condition.\n",
      "[6/282] (2.1%) Q: Äáº§u dÃ² Convex vÃ  Ä‘áº§u dÃ² Sector cá»§a mÃ¡y siÃªu Ã¢m ICU trong TD636 láº§n lÆ°á»£t cÃ³ dáº£i táº§n sá»‘ hoáº¡t Ä‘á»™ng bao nhiÃªu?\n",
      "âœ… Found 20 chunks (default search with top 20)\n",
      "Prompt TOKEN COUNT: 15970\n",
      "    â†’ ÄÃ¡p Ã¡n: C ğŸŸ¢ (61.3s)\n",
      "      Suy luáº­n: TÃ i liá»‡u 2 (Public_657.pdf) nÃªu rÃµ: \"Hoáº¡t Ä‘á»™ ng c á»§a Ä‘á»™ng cÆ¡ servo - mÃ u xÃ¡m (hi á»ƒ n th á»‹ ) khi Ä‘á»™ ng cÆ¡ servo táº¯ t vÃ  mÃ u l á»¥ c n áº¿u Ä‘á»™ng cÆ¡ báº­ t\". Äiá»u nÃ y xÃ¡c nháº­n biá»ƒu tÆ°á»£ng hoáº¡t Ä‘á»™ng cá»§a Ä‘á»™ng cÆ¡ servo hiá»ƒn thá»‹ mÃ u lá»¥c khi Ä‘ang báº­t.\n",
      "[7/282] (2.5%) Q: Trong tÃ i liá»‡u Public_665, cÃ³ bao nhiÃªu loáº¡i vÃ­t Ä‘Æ°á»£c liá»‡t kÃª trong danh sÃ¡ch chi tiáº¿t láº¯p xiáº¿t?\n",
      "âœ… Found 2 chunks (filtered by Public_665, searched top 30)\n",
      "Prompt TOKEN COUNT: 1517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error generating MCQ answer: Server error '500 Internal Server Error' for url 'http://127.0.0.1:8080/v1/chat/completions'\n",
      "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\VIETTEL_RACE\\qa-mcq-rag\\mcqaRAG_p4\\src\\services\\ollama\\ollamacpp_client.py\", line 70, in generate_mcq_answer\n",
      "    response = await self._generate(prompt)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\VIETTEL_RACE\\qa-mcq-rag\\mcqaRAG_p4\\src\\services\\ollama\\ollamacpp_client.py\", line 264, in _generate\n",
      "    response.raise_for_status()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"d:\\VIETTEL_RACE\\qa-mcq-rag\\mcqaRAG_p4\\.venv\\Lib\\site-packages\\httpx\\_models.py\", line 829, in raise_for_status\n",
      "    raise HTTPStatusError(message, request=request, response=self)\n",
      "httpx.HTTPStatusError: Server error '500 Internal Server Error' for url 'http://127.0.0.1:8080/v1/chat/completions'\n",
      "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500\n",
      "Error generating MCQ answer: Server error '500 Internal Server Error' for url 'http://127.0.0.1:8080/v1/chat/completions'\n",
      "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\VIETTEL_RACE\\qa-mcq-rag\\mcqaRAG_p4\\src\\services\\ollama\\ollamacpp_client.py\", line 70, in generate_mcq_answer\n",
      "    response = await self._generate(prompt)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\VIETTEL_RACE\\qa-mcq-rag\\mcqaRAG_p4\\src\\services\\ollama\\ollamacpp_client.py\", line 264, in _generate\n",
      "    response.raise_for_status()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"d:\\VIETTEL_RACE\\qa-mcq-rag\\mcqaRAG_p4\\.venv\\Lib\\site-packages\\httpx\\_models.py\", line 829, in raise_for_status\n",
      "    raise HTTPStatusError(message, request=request, response=self)\n",
      "httpx.HTTPStatusError: Server error '500 Internal Server Error' for url 'http://127.0.0.1:8080/v1/chat/completions'\n",
      "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500\n",
      "Error generating MCQ answer: Server error '500 Internal Server Error' for url 'http://127.0.0.1:8080/v1/chat/completions'\n",
      "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\VIETTEL_RACE\\qa-mcq-rag\\mcqaRAG_p4\\src\\services\\ollama\\ollamacpp_client.py\", line 70, in generate_mcq_answer\n",
      "    response = await self._generate(prompt)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\VIETTEL_RACE\\qa-mcq-rag\\mcqaRAG_p4\\src\\services\\ollama\\ollamacpp_client.py\", line 264, in _generate\n",
      "    response.raise_for_status()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"d:\\VIETTEL_RACE\\qa-mcq-rag\\mcqaRAG_p4\\.venv\\Lib\\site-packages\\httpx\\_models.py\", line 829, in raise_for_status\n",
      "    raise HTTPStatusError(message, request=request, response=self)\n",
      "httpx.HTTPStatusError: Server error '500 Internal Server Error' for url 'http://127.0.0.1:8080/v1/chat/completions'\n",
      "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500\n",
      "Error generating MCQ answer: Server error '500 Internal Server Error' for url 'http://127.0.0.1:8080/v1/chat/completions'\n",
      "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\VIETTEL_RACE\\qa-mcq-rag\\mcqaRAG_p4\\src\\services\\ollama\\ollamacpp_client.py\", line 70, in generate_mcq_answer\n",
      "    response = await self._generate(prompt)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\VIETTEL_RACE\\qa-mcq-rag\\mcqaRAG_p4\\src\\services\\ollama\\ollamacpp_client.py\", line 264, in _generate\n",
      "    response.raise_for_status()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"d:\\VIETTEL_RACE\\qa-mcq-rag\\mcqaRAG_p4\\.venv\\Lib\\site-packages\\httpx\\_models.py\", line 829, in raise_for_status\n",
      "    raise HTTPStatusError(message, request=request, response=self)\n",
      "httpx.HTTPStatusError: Server error '500 Internal Server Error' for url 'http://127.0.0.1:8080/v1/chat/completions'\n",
      "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    â†’ ÄÃ¡p Ã¡n: None ğŸ”´ (11.9s)\n",
      "      Suy luáº­n: Failed to generate answer\n",
      "[8/282] (2.8%) Q: VÄƒn báº£n Public_675 chá»§ yáº¿u mÃ´ táº£ vá» ná»™i dung gÃ¬?\n",
      "    â†’ ÄÃ¡p Ã¡n: None ğŸ”´ (12.2s)\n",
      "      Suy luáº­n: Failed to generate answer\n",
      "[9/282] (3.2%) Q: Theo cáº¥u trÃºc báº£ng trong TD648, cÃ¡c cá»™t thÃ´ng tin chÃ­nh gá»“m nhá»¯ng má»¥c nÃ o?\n",
      "\n",
      "    â†’ ÄÃ¡p Ã¡n: None ğŸ”´ (73.2s)\n",
      "      Suy luáº­n: Failed to generate answer\n",
      "[10/282] (3.5%) Q: VÄƒn báº£n Public 449 mÃ´ táº£ yÃªu cáº§u gÃ¬ vá» dá»¯ liá»‡u nháº­p má»›i? VÃ  bÆ°á»›c nÃ o cáº§n kÃ­ch hoáº¡t náº¿u dá»¯ liá»‡u khÃ´ng Ä‘áº¡t chuáº©n?\n",
      "    â†’ ÄÃ¡p Ã¡n: None ğŸ”´ (26.0s)\n",
      "      Suy luáº­n: Failed to generate answer\n",
      "[11/282] (3.9%) Q: VÃ­t cáº¥y A1-M12Ã—45 Ä‘Æ°á»£c quy Ä‘á»‹nh trong tiÃªu chuáº©n nÃ o?\n",
      "âœ… Found 1 chunks (filtered by Public_675, searched top 30)\n",
      "âœ… Found 20 chunks (default search with top 20)\n",
      "Prompt TOKEN COUNT: 657\n",
      "Prompt TOKEN COUNT: 13531\n",
      "âœ… Found 20 chunks (default search with top 20)\n",
      "Prompt TOKEN COUNT: 11490\n",
      "âœ… Found 20 chunks (default search with top 20)\n",
      "Prompt TOKEN COUNT: 10356\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"HÃ m chÃ­nh Ä‘á»ƒ cháº¡y toÃ n bá»™ pipeline.\"\"\"\n",
    "    MAX_WORKERS  = 4\n",
    "    print_header(\"ğŸš€ RAG MCQ SYSTEM - AUTOMATIC ANSWERING\")\n",
    "    print(f\"\\nğŸ“… Báº¯t Ä‘áº§u: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "    # Step 3: Check indexed documents\n",
    "    # doc_count = check_indexed_documents()\n",
    "    # if doc_count == 0:\n",
    "    #     print(\"\\nâŒ KhÃ´ng cÃ³ documents nÃ o trong há»‡ thá»‘ng!\")\n",
    "    #     print(\"KhÃ´ng thá»ƒ tráº£ lá»i cÃ¢u há»i. Vui lÃ²ng xá»­ lÃ½ PDF trÆ°á»›c!\")\n",
    "    #     return\n",
    "\n",
    "    # Step 4: Load questions\n",
    "    questions_df = load_questions()\n",
    "    if questions_df is None:\n",
    "        return\n",
    "\n",
    "    # print(questions_df)\n",
    "    # Step 5: Answer all questions\n",
    "    results_df = answer_all_questions(questions_df, max_workers=MAX_WORKERS)\n",
    "\n",
    "    # Step 6: Save results\n",
    "    save_results(results_df)\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d555e6b",
   "metadata": {},
   "source": [
    "## Usage Examples\n",
    "$ ollama-server -hf bartowski/SmallThinker-3B-Preview-GGUF\n",
    "\n",
    "### Option 1: Direct Generation (No Context)\n",
    "```python\n",
    "# Simple generation without context - fastest but least accurate\n",
    "result = ask_single_question_cpp(\n",
    "    question=\"What is machine learning?\",\n",
    "    options={\"A\": \"...\", \"B\": \"...\", \"C\": \"...\", \"D\": \"...\"},\n",
    "    context_chunks=None  # No context\n",
    ")\n",
    "```\n",
    "\n",
    "### Option 2: With Manual Context Retrieval\n",
    "```python\n",
    "# If you want to retrieve context yourself and then generate\n",
    "from src.services.opensearch.client import OpenSearchClient\n",
    "from src.services.embeddings.sentence_transformer import EmbeddingsService\n",
    "\n",
    "# Initialize retrieval services\n",
    "opensearch = OpenSearchClient(host=OPENSEARCH_HOST, ...)\n",
    "embeddings = EmbeddingsService(...)\n",
    "\n",
    "# Retrieve context\n",
    "query_embedding = await embeddings.embed_query(question)\n",
    "chunks = await opensearch.search_hybrid(query, query_embedding, top_k=30)\n",
    "\n",
    "# Generate with context\n",
    "result = ask_single_question_cpp(\n",
    "    question=\"What is machine learning?\",\n",
    "    options={\"A\": \"...\", \"B\": \"...\", \"C\": \"...\", \"D\": \"...\"},\n",
    "    context_chunks=chunks  # Retrieved context\n",
    ")\n",
    "```\n",
    "\n",
    "### Option 3: Full RAG Pipeline (Use API)\n",
    "```python\n",
    "# Let the API handle everything (retrieval + generation)\n",
    "result = ask_single_question_with_rag(\n",
    "    question=\"What is machine learning?\",\n",
    "    options={\"A\": \"...\", \"B\": \"...\", \"C\": \"...\", \"D\": \"...\"},\n",
    "    source_folder=\"folder_name\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe5c2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     try:\n",
    "#         main()\n",
    "#     except KeyboardInterrupt:\n",
    "#         print(\"\\n\\nâš ï¸  ÄÃ£ dá»«ng bá»Ÿi ngÆ°á»i dÃ¹ng (Ctrl+C)\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"\\n\\nâŒ Lá»—i khÃ´ng mong Ä‘á»£i: {e}\")\n",
    "#         import traceback\n",
    "#         # http://localhost:8000/api/v1/search/\n",
    "#         traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f03cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from pathlib import Path\n",
    "\n",
    "# DATA_DIR = Path(\"data\")\n",
    "# OUTPUT_FILE = DATA_DIR / \"answers_mcq_smallthinker.csv\"\n",
    "# # load results_df from OUTPUT_FILE\n",
    "# results_df = pd.read_csv(\n",
    "#     OUTPUT_FILE,\n",
    "#     on_bad_lines='skip',  # Skip lines with too many fields (pandas >= 1.3.0)\n",
    "#     # Alternatively, for older pandas: error_bad_lines=False\n",
    "#     sep=',',  # Explicitly set delimiter if not comma\n",
    "#     quotechar='\"',  # Ensure quotes are handled\n",
    "#     engine='python'  # Use Python parser for complex cases (slower but more flexible)\n",
    "# )\n",
    "\n",
    "# display_summary(results_df)\n",
    "\n",
    "# print_header(\"âœ… HOÃ€N Táº¤T!\")\n",
    "# print(f\"\\nğŸ“ Káº¿t quáº£ Ä‘Ã£ Ä‘Æ°á»£c lÆ°u táº¡i: {OUTPUT_FILE.absolute()}\")\n",
    "# print(f\"ğŸ“… Káº¿t thÃºc: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "# print(\"\\n\" + \"=\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369d28fb",
   "metadata": {},
   "source": [
    "### Test Llama.cpp API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f05aa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test llama.cpp server API\n",
    "# import requests\n",
    "# import json\n",
    "\n",
    "# # Test configuration\n",
    "# API_URL = 'http://127.0.0.1:8080/v1/chat/completions'\n",
    "# TEST_QUESTION = \"What is the capital of France?\"\n",
    "# TEST_OPTIONS = {\n",
    "#     \"A\": \"London\",\n",
    "#     \"B\": \"Paris\",\n",
    "#     \"C\": \"Berlin\",\n",
    "#     \"D\": \"Madrid\"\n",
    "# }\n",
    "\n",
    "# # Build test prompt\n",
    "# prompt = f\"\"\"Answer this multiple choice question:\n",
    "\n",
    "# Question: {TEST_QUESTION}\n",
    "\n",
    "# Options:\n",
    "# A. {TEST_OPTIONS['A']}\n",
    "# B. {TEST_OPTIONS['B']}\n",
    "# C. {TEST_OPTIONS['C']}\n",
    "# D. {TEST_OPTIONS['D']}\n",
    "\n",
    "# Provide your answer in this format:\n",
    "# ANSWER: [letter]\n",
    "# REASONING: [brief explanation]\n",
    "# CONFIDENCE: [high/medium/low]\n",
    "# \"\"\"\n",
    "\n",
    "# # Prepare request payload\n",
    "# payload = {\n",
    "#     \"model\": \"gpt-like-model\",\n",
    "#     \"messages\": [\n",
    "#         {\"role\": \"user\", \"content\": prompt}\n",
    "#     ],\n",
    "#     \"max_tokens\": 200,\n",
    "#     \"temperature\": 0.2\n",
    "# }\n",
    "\n",
    "# print(\"ğŸ” Testing llama.cpp server API...\")\n",
    "# print(f\"ğŸ“¡ URL: {API_URL}\")\n",
    "# print(f\"â“ Question: {TEST_QUESTION}\\n\")\n",
    "\n",
    "# try:\n",
    "#     # Send request\n",
    "#     response = requests.post(API_URL, json=payload, timeout=30)\n",
    "#     response.raise_for_status()\n",
    "\n",
    "#     # Parse response\n",
    "#     result = response.json()\n",
    "\n",
    "#     # Extract answer\n",
    "#     if \"choices\" in result and len(result[\"choices\"]) > 0:\n",
    "#         answer_content = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "#         print(\"âœ… Server Response:\")\n",
    "#         print(\"â”€\" * 80)\n",
    "#         print(answer_content)\n",
    "#         print(\"â”€\" * 80)\n",
    "\n",
    "#         # Show usage stats if available\n",
    "#         if \"usage\" in result:\n",
    "#             usage = result[\"usage\"]\n",
    "#             print(f\"\\nğŸ“Š Usage Stats:\")\n",
    "#             print(f\"  â€¢ Prompt tokens: {usage.get('prompt_tokens', 'N/A')}\")\n",
    "#             print(f\"  â€¢ Completion tokens: {usage.get('completion_tokens', 'N/A')}\")\n",
    "#             print(f\"  â€¢ Total tokens: {usage.get('total_tokens', 'N/A')}\")\n",
    "\n",
    "#         # Show timing if available\n",
    "#         if \"timings\" in result:\n",
    "#             timings = result[\"timings\"]\n",
    "#             print(f\"\\nâ±ï¸  Timing:\")\n",
    "#             print(f\"  â€¢ Prompt: {timings.get('prompt_ms', 0):.2f}ms\")\n",
    "#             print(f\"  â€¢ Predicted: {timings.get('predicted_ms', 0):.2f}ms\")\n",
    "#             print(f\"  â€¢ Speed: {timings.get('predicted_per_second', 0):.2f} tokens/sec\")\n",
    "#     else:\n",
    "#         print(\"âŒ No choices in response\")\n",
    "#         print(json.dumps(result, indent=2))\n",
    "\n",
    "# except requests.exceptions.ConnectionError:\n",
    "#     print(\"âŒ Connection Error: Cannot connect to server\")\n",
    "#     print(\"   Make sure llama-server is running on port 8080\")\n",
    "\n",
    "# except requests.exceptions.Timeout:\n",
    "#     print(\"âŒ Request timed out\")\n",
    "\n",
    "# except requests.exceptions.HTTPError as e:\n",
    "#     print(f\"âŒ HTTP Error: {e}\")\n",
    "#     print(f\"   Response: {response.text}\")\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(f\"âŒ Unexpected error: {e}\")\n",
    "#     import traceback\n",
    "#     traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd3e64e",
   "metadata": {},
   "source": [
    "## Test OllamaCppClient connect to Llama.cpp Directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cc66ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test OllamaCppClient directly\n",
    "# print(\"ğŸ§ª Testing OllamaCppClient...\")\n",
    "\n",
    "# test_question = \"What is the capital of France?\"\n",
    "# test_options = {\n",
    "#     \"A\": \"London\",\n",
    "#     \"B\": \"Paris\",\n",
    "#     \"C\": \"Berlin\",\n",
    "#     \"D\": \"Madrid\"\n",
    "# }\n",
    "\n",
    "# print(f\"\\nâ“ Question: {test_question}\")\n",
    "# print(f\"ğŸ“‹ Options: {test_options}\\n\")\n",
    "\n",
    "# # Test without context\n",
    "# print(\"=\" * 80)\n",
    "# print(\"Test 1: Direct generation (no context)\")\n",
    "# print(\"=\" * 80)\n",
    "\n",
    "# result = ask_single_question_cpp(\n",
    "#     question=test_question,\n",
    "#     options=test_options,\n",
    "#     context_chunks=None\n",
    "# )\n",
    "\n",
    "# print(f\"\\nâœ… Result:\")\n",
    "# print(f\"  Answer: {result.get('predicted_option')}\")\n",
    "# print(f\"  Confidence: {result.get('confidence')}\")\n",
    "# print(f\"  Reasoning: {result.get('reasoning')}\")\n",
    "# print(f\"  Model: {result.get('model')}\")\n",
    "\n",
    "# # Test with empty context (same as None)\n",
    "# print(\"\\n\" + \"=\" * 80)\n",
    "# print(\"Test 2: With empty context list\")\n",
    "# print(\"=\" * 80)\n",
    "\n",
    "# result2 = ask_single_question_cpp(\n",
    "#     question=test_question,\n",
    "#     options=test_options,\n",
    "#     context_chunks=[]\n",
    "# )\n",
    "\n",
    "# print(f\"\\nâœ… Result:\")\n",
    "# print(f\"  Answer: {result2.get('predicted_option')}\")\n",
    "# print(f\"  Confidence: {result2.get('confidence')}\")\n",
    "# print(f\"  Reasoning: {result2.get('reasoning')}\")\n",
    "\n",
    "# print(\"\\nâœ… Tests completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-mcq-system",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
